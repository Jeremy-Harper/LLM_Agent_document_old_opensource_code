# src/output_analyzer.py
import os
import logging
import json
import glob
from pathlib import Path
from openai import OpenAI
from src.llm_agent import LLMAgent

logger = logging.getLogger(__name__)

class OutputAnalyzer:
    def __init__(self, repo_path, api_key, model="gpt-4", temperature=0.2):
        self.repo_path = repo_path
        self.llm_agent = LLMAgent(api_key, model, temperature)
        
    def analyze_outputs(self):
        """Analyze the outputs of the repository"""
        logger.info(f"Analyzing outputs for repository at {self.repo_path}")
        
        try:
            # Get output file patterns
            output_patterns = self._identify_output_patterns()
            
            # Get example output files
            example_files = self._get_example_output_files(output_patterns)
            
            # Analyze output files
            output_files_analysis = self._analyze_output_files(example_files)
            
            # Create output dictionary
            output_analysis = {
                'patterns': output_patterns,
                'file_types': self._categorize_output_files(output_files_analysis),
                'files': output_files_analysis
            }
            
            logger.info(f"Output analysis complete. Found {len(output_files_analysis)} output file types.")
            return output_analysis
            
        except Exception as e:
            logger.error(f"Error analyzing outputs: {str(e)}")
            raise
    
    def _identify_output_patterns(self):
        """Identify patterns for output files"""
        output_patterns = []
        
        # Look for readme or documentation files
        readme_files = []
        for root, dirs, files in os.walk(self.repo_path):
            for file in files:
                if file.lower() in ['readme.md', 'readme.txt', 'documentation.md', 'docs.md']:
                    file_path = os.path.join(root, file)
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        try:
                            content = f.read()
                            readme_files.append({
                                'path': os.path.relpath(file_path, self.repo_path),
                                'content': content
                            })
                        except:
                            # Skip files that can't be read
                            continue
        
        # If readme files found, analyze them to find output patterns
        if readme_files:
            for readme in readme_files:
                prompt = f"""
                Analyze the following README/documentation file to identify patterns of output files that would be generated by this software:
                
                File: {readme['path']}
                
                ```
                {readme['content']}
                ```
                
                Please provide a JSON response with a list of output file patterns with the following structure:
                [
                    {{
                        "pattern": "<file pattern>",
                        "description": "<description of what this file contains>",
                        "format": "<file format if known>",
                        "importance": "critical/important/auxiliary"
                    }}
                ]
                
                Focus on identifying output files that would be generated during normal execution.
                """
                
                response = self.llm_agent.query(prompt)
                
                try:
                    patterns = json.loads(response)
                    output_patterns.extend(patterns)
                except:
                    logger.warning(f"Could not parse LLM response for output patterns: {readme['path']}")
        
        # If no patterns found from readme, use common output patterns
        if not output_patterns:
            output_patterns = [
                {"pattern": "*.txt", "description": "Text output files", "format": "text", "importance": "important"},
                {"pattern": "*.csv", "description": "CSV data files", "format": "csv", "importance": "critical"},
                {"pattern": "*.tsv", "description": "Tab-separated data files", "format": "tsv", "importance": "critical"},
                {"pattern": "*.log", "description": "Log files", "format": "text", "importance": "auxiliary"},
                {"pattern": "*.png", "description": "Plot images", "format": "image", "importance": "important"},
                {"pattern": "*.pdf", "description": "PDF reports", "format": "pdf", "importance": "important"},
                {"pattern": "output/*", "description": "Output directory files", "format": "various", "importance": "critical"},
                {"pattern": "results/*", "description": "Results directory files", "format": "various", "importance": "critical"}
            ]
        
        return output_patterns
    
    def _get_example_output_files(self, output_patterns):
        """Get example output files based on patterns"""
        example_files = []
        
        for pattern in output_patterns:
            # Convert pattern to glob pattern
            glob_pattern = os.path.join(self.repo_path, pattern['pattern'])
            
            # Find matching files
            matching_files = glob.glob(glob_pattern)
            
            # Take up to 3 examples of each pattern
            for file_path in matching_files[:3]:
                # Skip directories and binary files
                if os.path.isdir(file_path) or self._is_binary_file(file_path):
                    continue
                
                # Skip large files
                if os.path.getsize(file_path) > 100000:  # 100KB limit
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        try:
                            # Read just the first 50 lines
                            content = ''.join([next(f) for _ in range(50)])
                            content += "\n... [truncated] ..."
                        except:
                            # Skip files that can't be read
                            continue
                else:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        try:
                            content = f.read()
                        except:
                            # Skip files that can't be read
                            continue
                
                example_files.append({
                    'path': os.path.relpath(file_path, self.repo_path),
                    'pattern': pattern['pattern'],
                    'content': content,
                    'description': pattern['description'],
                    'format': pattern['format'],
                    'importance': pattern['importance']
                })
        
        return example_files
    
    def _is_binary_file(self, file_path):
        """Check if a file is binary"""
        try:
            with open(file_path, 'r') as f:
                f.read(1024)
                return False
        except UnicodeDecodeError:
            return True
    
    def _analyze_output_files(self, example_files):
        """Analyze output files to understand their structure and purpose"""
        output_files_analysis = []
        
        for file in example_files:
            prompt = f"""
            Analyze the following output file to understand its structure and purpose:
            
            File: {file['path']}
            Format: {file['format']}
            Description: {file['description']}
            
            Content sample:
            ```
            {file['content']}
            ```
            
            Please provide a JSON response with the following structure:
            {{
                "file_type": "<type of the file>",
                "purpose": "<purpose of this file>",
                "structure": "<description of the file structure>",
                "columns": [
                    {{
                        "name": "<column name>",
                        "description": "<what this column represents>",
                        "data_type": "<data type of this column>"
                    }}
                ],
                "interpretation_guide": "<how to interpret the data in this file>",
                "significance": "<why this file is important>",
                "related_files": ["<related file patterns>"]
            }}
            
            Focus on helping users understand what information is in this file and how to use it.
            """
            
            response = self.llm_agent.query(prompt)
            
            try:
                file_analysis = json.loads(response)
                file_analysis['path'] = file['path']
                file_analysis['pattern'] = file['pattern']
                file_analysis['importance'] = file['importance']
                output_files_analysis.append(file_analysis)
            except:
                logger.warning(f"Could not parse LLM response for file analysis: {file['path']}")
        
        return output_files_analysis
    
    def _categorize_output_files(self, output_files_analysis):
        """Categorize output files by type"""
        file_types = {}
        
        for file in output_files_analysis:
            file_type = file.get('file_type', 'unknown')
            if file_type in file_types:
                file_types[file_type].append(file['path'])
            else:
                file_types[file_type] = [file['path']]
        
        return file_types
